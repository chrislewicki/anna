# Discord Bot Token
# Get from: https://discord.com/developers/applications
DISCORD_TOKEN=your_discord_token_here

# ================================
# LLM Provider Configuration
# ================================

# Provider Selection
# Options: digitalocean (default), ollama-local, ollama-tailscale
LLM_PROVIDER=digitalocean

# System Prompt Configuration
# Global system prompt applied to all providers (can be overridden per provider)
# LLM_SYSTEM_PROMPT="You are Anna, a helpful Discord bot assistant."

# DigitalOcean GenAI Configuration (default provider)
# Note: AUTH_TOKEN is supported for backward compatibility
DIGITALOCEAN_AUTH_TOKEN=your_token_here
# DIGITALOCEAN_MODEL_URL=https://ppjmbaf3sh6p5tx2iaz53gmr.agents.do-ai.run/api/v1/chat/completions
# DIGITALOCEAN_MODEL=mistral
# DIGITALOCEAN_SYSTEM_PROMPT="You are Anna, a helpful Discord bot."

# Ollama Local Configuration
# Requires: network_mode: host in docker-compose.yml
# Usage: Uncomment LLM_PROVIDER line below to enable
# LLM_PROVIDER=ollama-local
# OLLAMA_LOCAL_URL=http://localhost:11434
# OLLAMA_LOCAL_MODEL=mistral
# OLLAMA_LOCAL_SYSTEM_PROMPT="You are Anna, a Discord bot. Be helpful and concise."

# Ollama Tailscale Configuration
# Requires: Running ollama instance accessible via Tailscale
# Usage: Uncomment LLM_PROVIDER line below and set your Tailscale URL
# LLM_PROVIDER=ollama-tailscale
# OLLAMA_TAILSCALE_URL=http://your-hostname.tail-scale.ts.net:11434
# OLLAMA_TAILSCALE_MODEL=mistral
# OLLAMA_TAILSCALE_SYSTEM_PROMPT="You are Anna, a Discord bot. Be helpful and concise."

# Legacy support (maps to DIGITALOCEAN_AUTH_TOKEN)
AUTH_TOKEN=your_auth_token_here
